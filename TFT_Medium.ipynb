{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUjq9sF05SI1GX4t/ISHeo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/os-angel/TFT/blob/main/TFT_Medium.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Temporal Fusion Transformers**\n",
        "\n",
        "Es una arquitectura Deel Learning muy pontente, generalmente usada cuando se quiere capturar dependencias a largo plazo e incluir la influiencia de covariables en el pasado y en el futuro y agregarle interpretabilidad al modelo creado. Generalemte dan buenos resultados comparado con estructuras como ARIMA, RNN o LSTM.\n",
        "\n",
        "El objetivo de este tutorial es:\n",
        "\n",
        "* Entender la arquitectura de un Temporal Fusion Transformer.\n",
        "* Implementar un TFT desde cero.\n",
        "* Ajustar el modelo TFT mediante Fine Tuning a un caso específico.\n",
        "\n",
        "\n",
        "### **Orden lógico del tutorial**\n",
        "Este notebook está ordenado por 11 pasos fundamentales para la implementación de un TFT, en el siguiente orden:\n",
        "\n",
        "1. Carga y preparación de los datos\n",
        "2. Preprocesamiento\n",
        "3. Definición del modelo TFT\n",
        "4. Entrenamiento y evaluación\n",
        "5. Predicción y visualización\n",
        "6. Interpretabilidad del modelo"
      ],
      "metadata": {
        "id": "xftdvCYCgF91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Carga y preparación de los datos**\n",
        "\n",
        "Es importante instalar la librería darts y que el entorjo de ejecución sea a través de la GPU para poder paralelizar las ejecuciones y que sea todo muy eficiente."
      ],
      "metadata": {
        "id": "Dor8KDpsgJnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalamos la librería dats\n",
        "!pip install darts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xNbL1h07gGSn",
        "outputId": "ce32e096-8995-4d88-dcb6-088714a3de3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: darts in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Requirement already satisfied: holidays>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from darts) (0.69)\n",
            "Requirement already satisfied: joblib>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.4.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from darts) (3.10.0)\n",
            "Requirement already satisfied: narwhals>=1.25.1 in /usr/local/lib/python3.11/dist-packages (from darts) (1.33.0)\n",
            "Requirement already satisfied: nfoursid>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from darts) (2.0.0)\n",
            "Requirement already satisfied: pmdarima>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.0.4)\n",
            "Requirement already satisfied: pyod>=0.9.5 in /usr/local/lib/python3.11/dist-packages (from darts) (2.0.4)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from darts) (1.14.1)\n",
            "Requirement already satisfied: shap>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from darts) (0.47.1)\n",
            "Requirement already satisfied: statsforecast>=1.4 in /usr/local/lib/python3.11/dist-packages (from darts) (2.0.1)\n",
            "Requirement already satisfied: statsmodels>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from darts) (0.14.4)\n",
            "Requirement already satisfied: tbats>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from darts) (1.1.3)\n",
            "Requirement already satisfied: tqdm>=4.60.0 in /usr/local/lib/python3.11/dist-packages (from darts) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from darts) (4.13.0)\n",
            "Requirement already satisfied: xarray>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2025.1.2)\n",
            "Requirement already satisfied: xgboost>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from darts) (2.1.4)\n",
            "Requirement already satisfied: pytorch-lightning>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.5.1)\n",
            "Requirement already satisfied: tensorboardX>=2.1 in /usr/local/lib/python3.11/dist-packages (from darts) (2.6.2.2)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from darts) (2.6.0+cu124)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from holidays>=0.11.1->darts) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->darts) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->darts) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->darts) (2025.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima>=1.8.0->darts) (3.0.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima>=1.8.0->darts) (2.3.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima>=1.8.0->darts) (75.2.0)\n",
            "Requirement already satisfied: numba>=0.51 in /usr/local/lib/python3.11/dist-packages (from pyod>=0.9.5->darts) (0.60.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.5.0->darts) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (2025.3.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.5.0->darts) (1.7.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning>=1.5.0->darts) (0.14.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->darts) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->darts) (3.6.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap>=0.40.0->darts) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap>=0.40.0->darts) (3.1.1)\n",
            "Requirement already satisfied: coreforecast>=0.0.12 in /usr/local/lib/python3.11/dist-packages (from statsforecast>=1.4->darts) (0.0.16)\n",
            "Requirement already satisfied: fugue>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from statsforecast>=1.4->darts) (0.9.1)\n",
            "Requirement already satisfied: utilsforecast>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from statsforecast>=1.4->darts) (0.2.12)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.14.0->darts) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX>=2.1->darts) (5.29.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->darts) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->darts) (1.3.0)\n",
            "Collecting pandas>=1.0.5 (from darts)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (3.11.15)\n",
            "Requirement already satisfied: triad>=0.9.7 in /usr/local/lib/python3.11/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.9.8)\n",
            "Requirement already satisfied: adagio>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fugue>=0.8.1->statsforecast>=1.4->darts) (0.2.6)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51->pyod>=0.9.5->darts) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->holidays>=0.11.1->darts) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->darts) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=1.5.0->darts) (1.18.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (18.1.0)\n",
            "Requirement already satisfied: fs in /usr/local/lib/python3.11/dist-packages (from triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (2.4.16)\n",
            "Requirement already satisfied: appdirs~=1.4.3 in /usr/local/lib/python3.11/dist-packages (from fs->triad>=0.9.7->fugue>=0.8.1->statsforecast>=1.4->darts) (1.4.4)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.0\n",
            "    Uninstalling pandas-2.0.0:\n",
            "      Successfully uninstalled pandas-2.0.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importamos las librerías necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from darts.models import TFTModel\n",
        "from darts import TimeSeries\n",
        "from darts.metrics import rmse\n",
        "from darts.explainability import TFTExplainer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from google.colab import drive\n",
        "from darts.dataprocessing.transformers import Scaler\n",
        "import itertools\n",
        "import torch"
      ],
      "metadata": {
        "id": "ekGvSYCXilfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **2. Definición del modelo TFT**\n",
        "Para aplicar el modelo, es necesario conocer algunos parámetros importantes y cómo afectan en el rendimiento del mismo:\n",
        "\n",
        "* **Input Chunk Length:** permite seleccionar el tamaño de la data histórica que se usa cono entrada en el modelo. Este parámetro es importante para darle el mayor contexto posible al modelo y que pueda generar pronósticos más precisos.\n",
        "\n",
        "* **Output Chunk Length:** indica la cantidad de predicciones u horizonte de predicciones se deben realizar ($\\tau$), por ejemplo, si colocamos 24 en este modelo, representa 24 predicciones en la ventana de tiempo establecida.\n",
        "\n",
        "* **Hidden size:** representa la cantidad de unidades ocultas dentro de la capa del LSTM, este parámetro influye en la capacidad de aprender patrones complejos. A medida que aumentamos la cantidad de unidades ocultas, también aumenta el coste computacional para procesar la información.\n",
        "\n",
        "* **LSTM Layers:** determina el número de capas LSTM en el modelo. Por ejemplo, si colocamos 2 capas, el modelo puede aprender mejor una estructura de datos jerárquicos (long-term dependencies). Añadir más capas pueden mejorar la precisión pero también incremente la complejidad del modelo y el tiempo de entrenamiento.\n",
        "\n",
        "* **Number of Attention Heads:** configura la cantidad de attention heads en el mecanismo Multi-head-Attention. Este parámetro mejora la capacidad del modelo de aprender y mejora la precisión en el pronóstico.\n",
        "\n",
        "* **Dropout Rate:** es una técnica que permite el overfitting mediante una desactivación de algunas unidades de forma aleatoria durante el entrenamiento, ayudando a generalizar mejor ante nuevos datos.\n",
        "\n",
        "* **Batch Size:** determina cuántas muestras puede manejar el modelo antes de actualizar sus pesos. p.ej. un batch size de 64 indica que el modelo puede procesar 64 muestras juntas. También es importante tomar en cuenta que este parámetro balancea la velocidad de entrenamiento y el uso de memoria para asegurar la eficiencia de entrenamiento.\n",
        "\n",
        "* **Number of epochs:** Determina cuántas veces el modelo necesita ver el dataset de entrenamiento (aunque es de forma aleatoria), un valor de 10 significa que el modelo iterará sobre el dataset 10 veces. A medida que aumentamos los epochs, el performance del modelo lo hace también pero debemos tener cuidado de no caer en overfitting.\n",
        "\n",
        "* **Static Covariates**: Este ajuste nos permite incluir variables estáticas, es decir, que permanecen constantes a medida que pasa el tiempo."
      ],
      "metadata": {
        "id": "F9Txp6bggbdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos los datos a analizar\n",
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/electricity.csv', index_col='ds', parse_dates=True)\n",
        "series = TimeSeries.from_dataframe(df, value_cols='y', fill_missing_dates=True, freq='H')\n",
        "# Cargamos covariables pasadas\n",
        "X_past = df[['Exogenous1', 'Exogenous2']]\n",
        "covariates = TimeSeries.from_dataframe(X_past, fill_missing_dates=True, freq='H')\n",
        "\n",
        "# Cargamos covariables futuras\n",
        "future_df = pd.read_csv('/content/drive/MyDrive/Datasets/electricity-future.csv', index_col='ds', parse_dates=True)\n",
        "X_future = future_df[['Exogenous1', 'Exogenous2']]\n",
        "X = pd.concat([X_past, X_future])\n",
        "future_covariates = TimeSeries.from_dataframe(X, fill_missing_dates=True, freq='H')"
      ],
      "metadata": {
        "id": "kZrr3psugZ0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizamos los datos\n",
        "scaler1, scaler2 = Scaler(), Scaler()\n",
        "y_transformed = scaler1.fit_transform(series)\n",
        "past_covariates_transformed = scaler2.fit_transform(covariates)\n",
        "future_covariates_transformed = scaler2.transform(future_covariates)"
      ],
      "metadata": {
        "id": "LSWK5R-hgs2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos una función para Optuna\n",
        "def objective(trial):\n",
        "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128)\n",
        "    lstm_layers = trial.suggest_int(\"lstm_layers\", 1, 4)\n",
        "    num_attention_heads = trial.suggest_int(\"num_attention_heads\", 2, 8)\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
        "    n_epochs = trial.suggest_int(\"n_epochs\", 10, 50)\n",
        "\n",
        "    # Creamos el TFT con hiperparámetros a optimizar con Optuna\n",
        "    model = TFTModel(\n",
        "        input_chunk_length=96,\n",
        "        output_chunk_length=24,\n",
        "        hidden_size=hidden_size,\n",
        "        lstm_layers=lstm_layers,\n",
        "        num_attention_heads=num_attention_heads,\n",
        "        dropout=dropout,\n",
        "        batch_size=batch_size,\n",
        "        n_epochs=n_epochs,\n",
        "        add_encoders={'cyclic': {'future': ['hour', 'day', 'month']}},\n",
        "        use_static_covariates=True,\n",
        "        pl_trainer_kwargs={\"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\", \"devices\": 1},\n",
        "    )\n",
        "\n",
        "    # Entrenamos el modelo\n",
        "    model.fit(y_transformed, past_covariates_transformed, future_covariates_transformed)\n",
        "\n",
        "    # Validamos con datos históricos\n",
        "    cv = model.historical_forecasts(\n",
        "        y_transformed,\n",
        "        past_covariates_transformed,\n",
        "        future_covariates_transformed,\n",
        "        forecast_horizon=24,\n",
        "        start=df.shape[0] - 10*24,\n",
        "        stride=24,\n",
        "        retrain=True\n",
        "    )\n",
        "\n",
        "    # Calculamos RMSE\n",
        "    rmse_value = np.mean([\n",
        "        np.sqrt(mean_squared_error(df.y[pred.index], scaler1.inverse_transform(pred).pd_series())) for pred in cv\n",
        "    ])\n",
        "\n",
        "    return rmse_value  # Minimizaremos RMSE\n"
      ],
      "metadata": {
        "id": "Ado_PaifgwaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutamos Optuna con 50 iteracioes\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# Mostramos los mejores hiperparámetros\n",
        "print(f\"Mejor RMSE: {study.best_value}\")\n",
        "print(f\"Mejores parámetros: {study.best_params}\")\n",
        "\n",
        "# Entrenamos el modelo con los mejores hiperparámetros\n",
        "best_params = study.best_params\n",
        "final_model = TFTModel(\n",
        "    input_chunk_length=96,\n",
        "    output_chunk_length=24,\n",
        "    hidden_size=best_params[\"hidden_size\"],\n",
        "    lstm_layers=best_params[\"lstm_layers\"],\n",
        "    num_attention_heads=best_params[\"num_attention_heads\"],\n",
        "    dropout=best_params[\"dropout\"],\n",
        "    batch_size=best_params[\"batch_size\"],\n",
        "    n_epochs=best_params[\"n_epochs\"],\n",
        "    add_encoders={'cyclic': {'future': ['hour', 'day', 'month']}},\n",
        "    use_static_covariates=True,\n",
        "    pl_trainer_kwargs={\"accelerator\": \"gpu\" if torch.cuda.is_available() else \"cpu\", \"devices\": 1},\n",
        ")\n",
        "\n",
        "final_model.fit(y_transformed, past_covariates_transformed, future_covariates_transformed)"
      ],
      "metadata": {
        "id": "BXpdpMargzub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el modelo entrenado\n",
        "final_model.save(\"tft_best_model.pth\")\n",
        "print(\"Modelo guardado como tft_best_model.pth\")\n",
        "\n",
        "# Realizamos las predicciones\n",
        "forecast = final_model.predict(n=24, past_covariates=past_covariates_transformed, future_covariates=future_covariates_transformed)\n",
        "forecast = TimeSeries.pd_series(scaler1.inverse_transform(forecast)).rename('TFT')\n",
        "\n",
        "# Graficamos los resultados\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(df.y['2017-12':], label='Actuals')\n",
        "plt.plot(forecast, label='Forecast', linestyle='dashed')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Electricity Consumption')\n",
        "plt.title('Forecast Result')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gwX9u8lyg2RO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}